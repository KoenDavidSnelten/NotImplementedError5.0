import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch
# from TweetNormalizer import normalizeTweet
import sys
from sklearn.preprocessing import LabelEncoder

# Create function to transform the labels to numbers
def transform_labels(tweet, encoder):
  l = encoder.transform([tweet['offensive_aggregated']])
  tweet['label'] = l[0]
  return tweet

def eval(infile, bert):
    # load models
    tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')
    model = AutoModel.from_pretrained(bert)  # PyTorch
    # load dev_data_text
    df_test = pd.read_csv(infile)
    # tokenize
    for tweet in df_test:
        norm = tweet.replace('URL', 'HTTPURL')
        tokenized = tokenizer(norm, padding="max_length",
                              truncation=True)
        
    # Encode the labels
    # le = LabelEncoder()
    # le.fit(df_test['train']['offensive_aggregated'])

    # df_test = df_test.map(lambda x: transform_labels(x, le))

    # predict labels
    model.eval()
    with torch.no_grad():
        output = model(tokenized)
    _, predicted = torch.max(output, 1)
    # make output file
    df_test["abusive_offensive_not"] = predicted
    df_test.to_csv('test_set_out.csv', index=False)


if __name__ == "__main__":
    eval(sys.argv[1], sys.argv[2])
